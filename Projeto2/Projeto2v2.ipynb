{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(categories):\n",
    "    # Training and Testing\n",
    "    data             = []\n",
    "    target           = []\n",
    "    size_image       = 32\n",
    "    img_dir          = 'train'    \n",
    "    \n",
    "    for idx, category in enumerate(categories):\n",
    "        img_dir_aux = img_dir + '/'+ category\n",
    "        data_path   = os.path.join(img_dir_aux, '*g')\n",
    "        files       = glob.glob(data_path)\n",
    "        for f1 in files:\n",
    "            img        = cv2.imread(f1, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resize = cv2.resize(img, (size_image, size_image), interpolation = cv2.INTER_AREA)\n",
    "            img_resize = img_to_array(img_resize)\n",
    "            \n",
    "            data.append(img_resize)\n",
    "            target.append(idx)\n",
    "        \n",
    "    target     = keras.utils.to_categorical(target, len(categories))    \n",
    "    data       = np.array(data)\n",
    "    target     = np.array(target)    \n",
    "    data       = data.astype(np.uint8)/255.0 \n",
    "    \n",
    "    aux          = np.arange(len(data))\n",
    "    np.random.shuffle(aux)\n",
    "    train_data   = data[aux]\n",
    "    train_target = target[aux]\n",
    "        \n",
    "    # Validation\n",
    "    val_data   = []\n",
    "    val_target = []\n",
    "    img_dir    = 'val'\n",
    "    for idx, category in enumerate(categories):        \n",
    "        img_dir_aux = img_dir + '/'+ category\n",
    "        data_path   = os.path.join(img_dir_aux, '*g')\n",
    "        files       = glob.glob(data_path)        \n",
    "        for f1 in files:\n",
    "            img        = cv2.imread(f1, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resize = cv2.resize(img, (size_image, size_image), interpolation = cv2.INTER_AREA)\n",
    "            img_resize = img_to_array(img_resize)\n",
    "            val_data.append(img_resize)\n",
    "            val_target.append(idx)\n",
    "    \n",
    "    val_target   = keras.utils.to_categorical(val_target, len(categories))                \n",
    "    val_data     = np.array(val_data)\n",
    "    \n",
    "    val_data =val_data.astype(np.uint8)/255.0\n",
    "    \n",
    "    val_target   = np.array(val_target)        \n",
    "    return train_data, train_target, val_data, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape[1:]\n",
      "(32, 32, 1)\n",
      "2910\n",
      "2910\n"
     ]
    }
   ],
   "source": [
    "batch_size        = 32\n",
    "num_classes       = 7\n",
    "save_dir          = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name        = 'faces.h5'\n",
    "categories  = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "# categories  = ['angry','disgust']\n",
    "# train_data, train_target, test_data, test_target, val_data, val_target = load_data(categories)\n",
    "train_data, train_target, val_data, val_target = load_data(categories)\n",
    "\n",
    "print('train_data.shape[1:]')\n",
    "print(train_data.shape[1:])\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs            = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "(1000, 32, 32, 1)\n",
      "WARNING:tensorflow:From /home/alexander/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2910 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "2910/2910 [==============================] - 11s 4ms/step - loss: 3.0812 - acc: 0.1993 - val_loss: 1.8032 - val_acc: 0.2680\n",
      "Epoch 2/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 2.0398 - acc: 0.2223 - val_loss: 1.7766 - val_acc: 0.4670\n",
      "Epoch 3/50\n",
      "2910/2910 [==============================] - 6s 2ms/step - loss: 1.8875 - acc: 0.2663 - val_loss: 1.8017 - val_acc: 0.3590\n",
      "Epoch 4/50\n",
      "2910/2910 [==============================] - 11s 4ms/step - loss: 1.8008 - acc: 0.3086 - val_loss: 1.5629 - val_acc: 0.4750\n",
      "Epoch 5/50\n",
      "2910/2910 [==============================] - 9s 3ms/step - loss: 1.7439 - acc: 0.3282 - val_loss: 1.5915 - val_acc: 0.4700\n",
      "Epoch 6/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.6928 - acc: 0.3540 - val_loss: 1.4499 - val_acc: 0.5350\n",
      "Epoch 7/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.6533 - acc: 0.3612 - val_loss: 1.4811 - val_acc: 0.4940\n",
      "Epoch 8/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.6043 - acc: 0.3784 - val_loss: 1.5115 - val_acc: 0.5290\n",
      "Epoch 9/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 1.5607 - acc: 0.4062 - val_loss: 1.5679 - val_acc: 0.4750\n",
      "Epoch 10/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.5183 - acc: 0.4241 - val_loss: 1.6460 - val_acc: 0.4250\n",
      "Epoch 11/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.4745 - acc: 0.4388 - val_loss: 1.4031 - val_acc: 0.5080\n",
      "Epoch 12/50\n",
      "2910/2910 [==============================] - 11s 4ms/step - loss: 1.4627 - acc: 0.4385 - val_loss: 1.3298 - val_acc: 0.5430\n",
      "Epoch 13/50\n",
      "2910/2910 [==============================] - 9s 3ms/step - loss: 1.3854 - acc: 0.4722 - val_loss: 1.3009 - val_acc: 0.5470\n",
      "Epoch 14/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.4054 - acc: 0.4701 - val_loss: 1.7663 - val_acc: 0.3020\n",
      "Epoch 15/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.3832 - acc: 0.4790 - val_loss: 1.2766 - val_acc: 0.5780\n",
      "Epoch 16/50\n",
      "2910/2910 [==============================] - 9s 3ms/step - loss: 1.3642 - acc: 0.4897 - val_loss: 1.2242 - val_acc: 0.5820\n",
      "Epoch 17/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.3182 - acc: 0.5086 - val_loss: 1.5230 - val_acc: 0.4790\n",
      "Epoch 18/50\n",
      "2910/2910 [==============================] - 13s 4ms/step - loss: 1.2459 - acc: 0.5313 - val_loss: 1.3522 - val_acc: 0.5200\n",
      "Epoch 19/50\n",
      "2910/2910 [==============================] - 12s 4ms/step - loss: 1.2532 - acc: 0.5381 - val_loss: 1.2370 - val_acc: 0.5850\n",
      "Epoch 20/50\n",
      "2910/2910 [==============================] - 9s 3ms/step - loss: 1.2158 - acc: 0.5457 - val_loss: 1.2163 - val_acc: 0.5610\n",
      "Epoch 21/50\n",
      "2910/2910 [==============================] - 11s 4ms/step - loss: 1.1745 - acc: 0.5715 - val_loss: 1.2813 - val_acc: 0.5650\n",
      "Epoch 22/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.1861 - acc: 0.5763 - val_loss: 1.2229 - val_acc: 0.5800\n",
      "Epoch 23/50\n",
      "2910/2910 [==============================] - 9s 3ms/step - loss: 1.1188 - acc: 0.5907 - val_loss: 1.1927 - val_acc: 0.6110\n",
      "Epoch 24/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 1.1121 - acc: 0.5866 - val_loss: 1.1288 - val_acc: 0.5970\n",
      "Epoch 25/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 1.0730 - acc: 0.6086 - val_loss: 1.2500 - val_acc: 0.5620\n",
      "Epoch 26/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 1.0302 - acc: 0.6220 - val_loss: 1.2463 - val_acc: 0.5360\n",
      "Epoch 27/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 1.0121 - acc: 0.6268 - val_loss: 1.1623 - val_acc: 0.5980\n",
      "Epoch 28/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.9753 - acc: 0.6460 - val_loss: 1.0826 - val_acc: 0.6120\n",
      "Epoch 29/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.9676 - acc: 0.6519 - val_loss: 1.2711 - val_acc: 0.5470\n",
      "Epoch 30/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.9443 - acc: 0.6605 - val_loss: 1.2894 - val_acc: 0.5760\n",
      "Epoch 31/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.9318 - acc: 0.6557 - val_loss: 1.1212 - val_acc: 0.6080\n",
      "Epoch 32/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 0.8841 - acc: 0.6804 - val_loss: 1.2271 - val_acc: 0.6080\n",
      "Epoch 33/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.8956 - acc: 0.6804 - val_loss: 1.2612 - val_acc: 0.5540\n",
      "Epoch 34/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.8655 - acc: 0.6825 - val_loss: 1.2173 - val_acc: 0.5830\n",
      "Epoch 35/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.8394 - acc: 0.7003 - val_loss: 1.3121 - val_acc: 0.5180\n",
      "Epoch 36/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.8233 - acc: 0.7034 - val_loss: 1.1057 - val_acc: 0.6120\n",
      "Epoch 37/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.7626 - acc: 0.7158 - val_loss: 1.0923 - val_acc: 0.6320\n",
      "Epoch 38/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.7331 - acc: 0.7443 - val_loss: 1.2008 - val_acc: 0.6240\n",
      "Epoch 39/50\n",
      "2910/2910 [==============================] - 6s 2ms/step - loss: 0.7489 - acc: 0.7234 - val_loss: 1.2402 - val_acc: 0.6100\n",
      "Epoch 40/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.7386 - acc: 0.7381 - val_loss: 1.3119 - val_acc: 0.5540\n",
      "Epoch 41/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 0.7484 - acc: 0.7368 - val_loss: 1.2258 - val_acc: 0.5840\n",
      "Epoch 42/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.7153 - acc: 0.7430 - val_loss: 1.1762 - val_acc: 0.6040\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.7236 - acc: 0.7430 - val_loss: 1.2862 - val_acc: 0.5930\n",
      "Epoch 44/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 0.6533 - acc: 0.7722 - val_loss: 1.1389 - val_acc: 0.6420\n",
      "Epoch 45/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.6577 - acc: 0.7625 - val_loss: 1.2170 - val_acc: 0.6270\n",
      "Epoch 46/50\n",
      "2910/2910 [==============================] - 7s 2ms/step - loss: 0.6254 - acc: 0.7852 - val_loss: 1.1598 - val_acc: 0.6070\n",
      "Epoch 47/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 0.6088 - acc: 0.7797 - val_loss: 1.2555 - val_acc: 0.6150\n",
      "Epoch 48/50\n",
      "2910/2910 [==============================] - 7s 3ms/step - loss: 0.6160 - acc: 0.7832 - val_loss: 1.2492 - val_acc: 0.6110\n",
      "Epoch 49/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 0.5909 - acc: 0.7969 - val_loss: 1.2339 - val_acc: 0.6080\n",
      "Epoch 50/50\n",
      "2910/2910 [==============================] - 8s 3ms/step - loss: 0.5614 - acc: 0.8055 - val_loss: 1.1985 - val_acc: 0.6490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe80809bfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation = 'relu', padding='same',input_shape=train_data.shape[1:]))\n",
    "model.add(Conv2D(32, (5, 5), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "# ACHF - se cambia el nombre del parametro, nueva documentacion\n",
    "# opt = keras.optimizers.RMSprop(lr = 0.0001, decay = 1e-6)\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "# opt = keras.optimizers.RMSprop(lr = 0.001)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "batch_size = 32\n",
    "print(val_data.shape)\n",
    "model.fit(train_data,train_target,epochs = epochs, batch_size = batch_size,validation_data = (val_data, val_target), shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/alexander/Documents/Cursos/RN/Tareas/Projeto2/saved_models/facesv6.h5 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bca78533c7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Score trained model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "model_name        = 'facesv6.h5'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# train_data, train_target, test_data, test_target, val_data, val_target = load_data(categories)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(test_data, test_target, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "scores = model.evaluate(val_data, val_target, verbose=1)\n",
    "print('Val loss:', scores[0])\n",
    "print('Val accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu', input_shape=train_data.shape[1:], kernel_regularizer=l2(0.01)))\n",
    "# model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "# model.add(Dropout(0.5))    \n",
    "# model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.5))    \n",
    "# model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.5))    \n",
    "# model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.5))    \n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# adam = Adam(lr = learning_rate)\n",
    "# model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# batch_size = 32\n",
    "# print(val_data.shape)\n",
    "# model.fit(train_data,train_target,epochs = epochs, batch_size = batch_size,validation_data = (val_data, val_target), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
